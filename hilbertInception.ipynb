{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d453b605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/romantic-brutalist/hilbertcnn/0c3dd2d71a7b432188d328e9f81f7f2d\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import numpy as np\n",
    "from hilbert import decode, encode\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Create an experiment with your api key\n",
    "experiment = Experiment(\n",
    "    api_key=\"uM0HPEvEu6OyX3dTEuB4Fihgz\",\n",
    "    project_name=\"hilbertcnn\",\n",
    "    workspace=\"romantic-brutalist\",\n",
    ")\n",
    "import os\n",
    "os.environ[\"COMET_API_KEY\"] = \"uM0HPEvEu6OyX3dTEuB4Fihgz\"\n",
    "hyper_params = {\"batch_size\": 64, \"num_epochs\": 5, \"learning_rate\": 0.0015,\"trainset\":\"_4layer_w_volume\",\"loss_metric\":\"MSE\",\"regularizer\":\"L1\"}\n",
    "experiment.log_parameters(hyper_params)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "class HilbertImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_labels = pd.read_parquet(os.path.join(self.img_dir, annotations_file))\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, -1])\n",
    "        image = np.load(img_path)\n",
    "        label = self.img_labels.iloc[idx, :-1].values.astype(\"float64\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "train_ds = HilbertImageDataset(annotations_file=\"target_next_20.parquet\",img_dir=\"imgs/train\"+hyper_params[\"trainset\"]+\"/\")\n",
    "test_ds = HilbertImageDataset(annotations_file=\"target_next_20.parquet\",img_dir=\"imgs/test\"+hyper_params[\"trainset\"]+\"/\")\n",
    "trainloader = DataLoader(train_ds, batch_size=hyper_params[\"batch_size\"], shuffle=True)\n",
    "testloader = DataLoader(test_ds, batch_size=hyper_params[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a56d3ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 5, 32, 32]), torch.Size([64, 5, 16, 16]), torch.Size([64, 8]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c = next(iter(testloader))\n",
    "a.shape,b.shape,c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b17bd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asd/asdd/fc'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.join(\"asd\",\"asdd\",\"fc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fdb7f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "from typing import Any, Callable, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "\n",
    "__all__ = [\"Inception3\", \"InceptionOutputs\", \"_InceptionOutputs\", \"Inception_V3_Weights\", \"inception_v3\"]\n",
    "\n",
    "\n",
    "InceptionOutputs = namedtuple(\"InceptionOutputs\", [\"logits\", \"aux_logits\"])\n",
    "InceptionOutputs.__annotations__ = {\"logits\": Tensor, \"aux_logits\": Optional[Tensor]}\n",
    "\n",
    "# Script annotations failed with _GoogleNetOutputs = namedtuple ...\n",
    "# _InceptionOutputs set here for backwards compat\n",
    "_InceptionOutputs = InceptionOutputs\n",
    "\n",
    "\n",
    "class InceptionA(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, pool_features: int, conv_block: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)\n",
    "        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "    \n",
    "class InceptionB(nn.Module):\n",
    "    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch3x3 = conv_block(in_channels, 384, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, stride=2)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "\n",
    "        outputs = [branch3x3, branch3x3dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "    \n",
    "class InceptionC(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, channels_7x7: int, conv_block: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1x1 = conv_block(in_channels, 192, kernel_size=1)\n",
    "\n",
    "        c7 = channels_7x7\n",
    "        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7_3 = conv_block(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "\n",
    "        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7dbl_5 = conv_block(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "\n",
    "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch7x7 = self.branch7x7_1(x)\n",
    "        branch7x7 = self.branch7x7_2(branch7x7)\n",
    "        branch7x7 = self.branch7x7_3(branch7x7)\n",
    "\n",
    "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
    "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "class InceptionD(nn.Module):\n",
    "    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch3x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
    "        self.branch3x3_2 = conv_block(192, 320, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch7x7x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
    "        self.branch7x7x3_2 = conv_block(192, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7x3_3 = conv_block(192, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7x3_4 = conv_block(192, 192, kernel_size=3, stride=2)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = self.branch3x3_2(branch3x3)\n",
    "\n",
    "        branch7x7x3 = self.branch7x7x3_1(x)\n",
    "        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        outputs = [branch3x3, branch7x7x3, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionE(nn.Module):\n",
    "    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1x1 = conv_block(in_channels, 320, kernel_size=1)\n",
    "\n",
    "        self.branch3x3_1 = conv_block(in_channels, 384, kernel_size=1)\n",
    "        self.branch3x3_2a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3_2b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "        self.branch3x3dbl_1 = conv_block(in_channels, 448, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = conv_block(448, 384, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3dbl_3b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = [\n",
    "            self.branch3x3_2a(branch3x3),\n",
    "            self.branch3x3_2b(branch3x3),\n",
    "        ]\n",
    "        branch3x3 = torch.cat(branch3x3, 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = [\n",
    "            self.branch3x3dbl_3a(branch3x3dbl),\n",
    "            self.branch3x3dbl_3b(branch3x3dbl),\n",
    "        ]\n",
    "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, **kwargs: Any) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.leaky_relu(x, inplace=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23fb8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Conv2d_1a_3x3 = BasicConv2d(5, 32, kernel_size=3, stride=2)\n",
    "        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3)\n",
    "        self.Conv2d_2b_3x3 = BasicConv2d(32, 192, kernel_size=3, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.Mixed_5b = InceptionA(192, pool_features=32)\n",
    "        self.Mixed_5c = InceptionA(256, pool_features=64)\n",
    "        self.Mixed_5d = InceptionA(288, pool_features=64)\n",
    "        self.Mixed_6a = InceptionB(288)\n",
    "        self.Mixed_6b = InceptionC(768, channels_7x7=128)\n",
    "        self.Mixed_6c = InceptionC(768, channels_7x7=160)\n",
    "        self.Mixed_6d = InceptionC(768, channels_7x7=160)\n",
    "        self.Mixed_6e = InceptionC(768, channels_7x7=192)\n",
    "        self.Mixed_7a = InceptionD(768)\n",
    "        self.Mixed_7b = InceptionE(1280)\n",
    "        self.Mixed_7c = InceptionE(2048)\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.fc2 = nn.Linear(256, 32)\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # N x 32 x 149 x 149\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # N x 32 x 147 x 147\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # N x 64 x 147 x 147\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        # N x 64 x 73 x 73\n",
    "        # N x 192 x 35 x 35\n",
    "        x = self.Mixed_5b(x)\n",
    "        # N x 256 x 35 x 35\n",
    "        x = self.Mixed_5c(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_5d(x)\n",
    "        x2= torch.clone(x)\n",
    "        x = self.Mixed_6a(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6b(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6c(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6d(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6e(x)\n",
    "        x = self.avgpool(x)\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = self.dropout(x)\n",
    "        # N x 2048 x 1 x 1\n",
    "        #x2 = self.avgpool(x2)\n",
    "        # N x 2048 x 1 x 1\n",
    "        #x2 = self.dropout(x2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        #x2 = torch.flatten(x2, 1)\n",
    "        #x = torch.cat([x,x2],1)\n",
    "        # N x 2048\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return x\n",
    "net = Net().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c51ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f9dbefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=hyper_params[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09dd9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,criterion,dataloader,epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_loss= 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.8f}')\n",
    "            experiment.log_metric(\"batch_loss\", running_loss/200)\n",
    "            running_loss = 0.0\n",
    "    print(\"total loss:\",total_loss/len(trainloader))\n",
    "    experiment.log_metrics({\"loss\": total_loss/len(trainloader)}, epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "518a5328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.00001568\n",
      "[1,   400] loss: 0.00001935\n",
      "[1,   600] loss: 0.00001814\n",
      "[1,   800] loss: 0.00001646\n",
      "[1,  1000] loss: 0.00001848\n",
      "total loss: 1.7782919764206746e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning Model Training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, dataloader, epoch)\u001b[0m\n\u001b[1;32m     15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with experiment.train():\n",
    "    print(\"Running Model Training\")\n",
    "    for epoch in range(10):\n",
    "        train(net, optimizer, criterion, trainloader, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f2a38b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (Conv2d_1a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(5, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2b_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Mixed_5b): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5c): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5d): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6a): InceptionB(\n",
       "    (branch3x3): BasicConv2d(\n",
       "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6b): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6c): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6d): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6e): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7a): InceptionD(\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7b): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7c): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=8, bias=True)\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c54824a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml.integration.pytorch import log_model\n",
    "model_checkpoint = {\n",
    "    \"epoch\": epoch,\n",
    "    \"model_state_dict\": net.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"criterion\":criterion}\n",
    "log_model(experiment, model_checkpoint, model_name=\"4_layer_w_volume_inceptionv2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d954742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/caliban/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0948e-03,  1.2949e-03, -1.6751e-03,  2.5323e-04, -1.4569e-03,\n",
       "          2.0959e-03, -1.0867e-03,  1.9293e-03],\n",
       "        [-2.7205e-03,  2.3584e-03, -2.4516e-03,  1.1084e-03, -2.1599e-03,\n",
       "          2.5459e-03, -1.7366e-03,  2.3676e-03],\n",
       "        [-2.8981e-03,  2.4941e-03, -1.6756e-03, -4.2668e-04,  9.2305e-05,\n",
       "          2.9440e-03,  1.5331e-04,  3.9577e-03],\n",
       "        [-2.7792e-03,  2.4077e-03, -2.6093e-03,  3.7748e-04, -1.6751e-03,\n",
       "          2.5245e-03, -7.5998e-04,  3.0079e-03],\n",
       "        [-1.4529e-03,  4.7329e-04, -1.3567e-03,  6.1536e-04, -1.5599e-03,\n",
       "          1.5388e-03, -9.8765e-04,  1.6696e-03],\n",
       "        [-1.5847e-03,  8.1147e-04, -1.4492e-03,  7.7370e-04, -1.8984e-03,\n",
       "          1.7834e-03, -9.6627e-04,  1.5284e-03],\n",
       "        [-8.1979e-04,  4.5647e-04, -2.1742e-03,  2.4961e-03, -2.9153e-03,\n",
       "          1.0440e-03, -1.5904e-03,  1.2491e-03],\n",
       "        [-7.0731e-04,  4.9919e-06, -1.5441e-03,  2.7066e-03, -3.3062e-03,\n",
       "          5.5164e-05, -1.8395e-03,  6.3239e-04],\n",
       "        [-8.9446e-04,  7.2286e-05, -9.0763e-04,  1.3231e-03, -1.8450e-03,\n",
       "          9.5737e-04, -9.1370e-04,  1.0891e-03],\n",
       "        [-1.6216e-03,  9.6123e-04, -1.5455e-03,  1.2523e-03, -2.2513e-03,\n",
       "          1.4655e-03, -1.4488e-03,  2.2553e-03],\n",
       "        [-2.5138e-03,  2.1330e-03, -2.2982e-03,  5.5537e-04, -2.0806e-03,\n",
       "          2.5419e-03, -1.6328e-03,  2.9669e-03],\n",
       "        [-5.1811e-05, -8.5060e-04, -1.7026e-03,  2.5224e-03, -8.5216e-04,\n",
       "         -9.6393e-04, -6.3726e-04,  1.3699e-04],\n",
       "        [-1.1343e-03,  6.7875e-04, -2.2518e-03,  1.4704e-03, -2.3714e-03,\n",
       "          1.4852e-03, -1.1507e-03,  1.7671e-03],\n",
       "        [-1.8420e-03,  1.4779e-03, -1.3293e-03, -2.5541e-04, -7.9799e-04,\n",
       "          2.4199e-03, -5.1121e-04,  2.2969e-03],\n",
       "        [-2.0200e-04, -2.2748e-04, -1.8047e-03,  2.5335e-03,  1.3243e-04,\n",
       "         -1.1847e-03, -2.9845e-04, -4.9234e-04],\n",
       "        [-1.8517e-03,  1.3533e-03, -1.1889e-03,  5.4987e-04, -9.4937e-04,\n",
       "          1.8560e-03, -8.5046e-04,  2.1689e-03],\n",
       "        [-1.3682e-03,  1.0564e-03, -1.7586e-03,  1.3838e-03, -1.9128e-03,\n",
       "          1.4767e-03, -1.1184e-03,  1.4964e-03],\n",
       "        [-8.1719e-04,  2.3896e-04, -1.0329e-03,  9.9257e-04, -1.6484e-03,\n",
       "          9.4242e-04, -7.0602e-04,  1.2143e-03],\n",
       "        [-4.7245e-04, -2.8461e-06, -9.8468e-04,  9.1866e-04, -1.7728e-03,\n",
       "          4.5325e-04, -4.4399e-04,  8.4387e-04],\n",
       "        [-2.2644e-03,  1.9261e-03, -2.6419e-03,  2.3777e-03, -3.4182e-03,\n",
       "          2.2594e-03, -2.4547e-03,  2.2363e-03],\n",
       "        [-1.8872e-03,  1.6222e-03, -1.6805e-03,  1.0854e-03, -2.2059e-03,\n",
       "          1.7024e-03, -1.2928e-03,  1.9178e-03],\n",
       "        [ 2.6807e-04, -5.9645e-04, -8.4537e-04,  1.7783e-03, -1.7427e-03,\n",
       "          4.2218e-04, -4.6583e-04,  1.3459e-04],\n",
       "        [-1.2166e-03,  1.5585e-03, -2.0205e-03,  1.8903e-03, -2.8996e-03,\n",
       "          1.8108e-03, -1.4198e-03,  1.4461e-03],\n",
       "        [-1.9914e-03,  1.6335e-03, -2.3997e-03,  5.5727e-04, -1.2127e-03,\n",
       "          2.1662e-03, -1.0084e-03,  2.5186e-03],\n",
       "        [-1.8877e-03,  7.8222e-04, -1.8329e-03,  8.6163e-04, -2.1749e-03,\n",
       "          1.6907e-03, -1.0404e-03,  2.1942e-03],\n",
       "        [-2.0650e-03,  1.3343e-03, -1.5976e-03,  4.5690e-04, -1.7000e-03,\n",
       "          2.1040e-03, -1.0902e-03,  2.5018e-03],\n",
       "        [-1.3418e-03,  4.3459e-04, -8.5431e-04,  3.6305e-04, -1.4319e-03,\n",
       "          1.4878e-03, -7.2329e-04,  2.2880e-03],\n",
       "        [-1.0866e-03,  7.9560e-04, -1.1275e-03, -4.8384e-05, -1.7274e-03,\n",
       "          2.9049e-03, -2.2193e-03,  5.0152e-03],\n",
       "        [-1.2641e-03,  6.9343e-04, -1.9170e-03,  2.1436e-03, -2.8197e-03,\n",
       "          1.5372e-03, -1.4451e-03,  1.2317e-03],\n",
       "        [-3.4711e-04, -4.3446e-04, -7.2738e-04,  1.7980e-03, -2.2087e-03,\n",
       "          3.4563e-04, -1.2781e-03,  1.1335e-03],\n",
       "        [-1.3811e-03,  1.2297e-03, -1.5637e-03,  2.1912e-03, -2.9350e-03,\n",
       "          1.3659e-03, -1.2590e-03,  1.6109e-03],\n",
       "        [-1.2531e-03,  1.0653e-03, -1.2987e-03,  1.7387e-04, -1.2458e-03,\n",
       "          1.7034e-03, -2.2678e-04,  1.8833e-03],\n",
       "        [-1.0560e-03,  8.9288e-04, -1.3366e-03,  1.9255e-03, -2.8007e-03,\n",
       "          1.3869e-03, -1.7503e-03,  1.5571e-03],\n",
       "        [-1.8191e-04, -4.1853e-04, -1.7345e-03,  2.9407e-03,  1.4368e-04,\n",
       "         -1.7198e-03, -5.7770e-04, -2.5164e-04],\n",
       "        [-1.1260e-03,  1.2240e-03, -1.8447e-03,  5.4421e-04, -1.1508e-03,\n",
       "          1.6583e-03, -8.8367e-04,  1.4183e-03],\n",
       "        [-9.8485e-04,  5.2154e-05, -8.7094e-04,  2.0229e-03, -2.8746e-03,\n",
       "          5.4944e-04, -9.5370e-04,  9.9145e-04],\n",
       "        [-9.5057e-04,  6.2744e-04, -9.7196e-04,  3.1897e-04, -1.4901e-03,\n",
       "          1.3667e-03, -3.8725e-04,  1.2180e-03],\n",
       "        [-1.4957e-03,  8.6366e-04, -1.4013e-03,  3.5387e-04, -1.1533e-03,\n",
       "          1.9701e-03, -5.4809e-04,  2.1932e-03],\n",
       "        [-5.3450e-04,  1.2678e-03, -1.2825e-03, -4.1834e-04, -3.8847e-04,\n",
       "          1.6372e-03,  3.3221e-04,  1.3096e-03],\n",
       "        [-3.7123e-04, -8.3268e-05, -1.1642e-03,  1.1795e-03, -1.9427e-03,\n",
       "          2.9553e-04, -4.0385e-04,  3.5390e-04],\n",
       "        [-2.2150e-03,  2.1333e-03, -1.3588e-03, -2.8752e-04, -7.4130e-04,\n",
       "          3.0043e-03, -4.0458e-04,  3.0357e-03],\n",
       "        [-3.3773e-03,  2.7218e-03, -3.2345e-03,  1.2924e-03, -3.1148e-03,\n",
       "          3.3044e-03, -2.1649e-03,  3.8515e-03],\n",
       "        [-7.5796e-04,  3.3545e-04, -1.5008e-03,  1.7675e-03, -2.5892e-03,\n",
       "          8.7559e-04, -1.2738e-03,  8.3950e-04],\n",
       "        [-6.9885e-04, -3.5524e-05, -5.7664e-04,  8.0542e-04, -9.0770e-04,\n",
       "          5.4435e-04, -3.1254e-04,  1.5941e-03]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "cols = list(pd.concat([pd.read_parquet(\"imgs/train\"+hyper_params[\"trainset\"]+\"/target_next_20.parquet\"),\n",
    "pd.read_parquet(\"imgs/test\"+hyper_params[\"trainset\"]+\"/target_next_20.parquet\")]).iloc[:,:-1].columns)\n",
    "fcs = [[i+\"_target\",i+\"_pred\"] for i in cols]\n",
    "fullcols = [item for sublist in fcs for item in sublist]\n",
    "tl = []\n",
    "for images, labels in dataiter:\n",
    "    images = images.float().to(device)\n",
    "    labels = labels.float().to(device)\n",
    "    outputs = net(images)\n",
    "    evaldf = pd.DataFrame(labels.cpu(),columns=[i+\"_target\" for i in cols]).join(pd.DataFrame(outputs.cpu().tolist(),columns=[i+\"_pred\" for i in cols]))\n",
    "    tl.append(evaldf[fullcols])\n",
    "tf = pd.concat(tl)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e14bc4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Low_5_target</th>\n",
       "      <th>Low_5_pred</th>\n",
       "      <th>High_5_target</th>\n",
       "      <th>High_5_pred</th>\n",
       "      <th>Low_10_target</th>\n",
       "      <th>Low_10_pred</th>\n",
       "      <th>High_10_target</th>\n",
       "      <th>High_10_pred</th>\n",
       "      <th>Low_15_target</th>\n",
       "      <th>Low_15_pred</th>\n",
       "      <th>High_15_target</th>\n",
       "      <th>High_15_pred</th>\n",
       "      <th>Low_20_target</th>\n",
       "      <th>Low_20_pred</th>\n",
       "      <th>High_20_target</th>\n",
       "      <th>High_20_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.001336</td>\n",
       "      <td>-0.003495</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>-0.001328</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>-0.001297</td>\n",
       "      <td>-0.001542</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.003134</td>\n",
       "      <td>-0.001293</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.000684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.003528</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>0.001288</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>0.001148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.031683</td>\n",
       "      <td>-0.019887</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.002640</td>\n",
       "      <td>-0.050941</td>\n",
       "      <td>-0.009558</td>\n",
       "      <td>-0.016854</td>\n",
       "      <td>-0.007229</td>\n",
       "      <td>-0.066908</td>\n",
       "      <td>-0.020411</td>\n",
       "      <td>-0.043833</td>\n",
       "      <td>-0.003564</td>\n",
       "      <td>-0.059140</td>\n",
       "      <td>-0.006530</td>\n",
       "      <td>-0.039862</td>\n",
       "      <td>-0.005927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.001732</td>\n",
       "      <td>-0.005420</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>-0.002212</td>\n",
       "      <td>-0.000422</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>-0.002545</td>\n",
       "      <td>-0.003317</td>\n",
       "      <td>-0.000317</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>-0.002748</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>-0.000544</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000888</td>\n",
       "      <td>-0.002895</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>-0.000915</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>-0.000901</td>\n",
       "      <td>-0.000558</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>-0.000918</td>\n",
       "      <td>0.001975</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.000789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.000365</td>\n",
       "      <td>-0.001229</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.003297</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.003919</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>0.001416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.004704</td>\n",
       "      <td>0.043472</td>\n",
       "      <td>0.013630</td>\n",
       "      <td>0.025916</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.059302</td>\n",
       "      <td>0.006537</td>\n",
       "      <td>0.036149</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>0.009758</td>\n",
       "      <td>0.036223</td>\n",
       "      <td>0.006102</td>\n",
       "      <td>0.053114</td>\n",
       "      <td>0.004334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Low_5_target   Low_5_pred  High_5_target  High_5_pred  Low_10_target  \\\n",
       "count   7916.000000  7916.000000    7916.000000  7916.000000    7916.000000   \n",
       "mean      -0.001336    -0.003495       0.001387     0.002386      -0.001328   \n",
       "std        0.001608     0.003304       0.001755     0.001701       0.002745   \n",
       "min       -0.031683    -0.019887      -0.000036    -0.002640      -0.050941   \n",
       "25%       -0.001732    -0.005420       0.000389     0.001233      -0.002212   \n",
       "50%       -0.000888    -0.002895       0.000913     0.002191      -0.000915   \n",
       "75%       -0.000365    -0.001229       0.001784     0.003297       0.000078   \n",
       "max        0.000047     0.004704       0.043472     0.013630       0.025916   \n",
       "\n",
       "       Low_10_pred  High_10_target  High_10_pred  Low_15_target  Low_15_pred  \\\n",
       "count  7916.000000     7916.000000   7916.000000    7916.000000  7916.000000   \n",
       "mean      0.000308        0.001413      0.001496      -0.001297    -0.001542   \n",
       "std       0.001451        0.002813      0.001398       0.003449     0.003528   \n",
       "min      -0.009558       -0.016854     -0.007229      -0.066908    -0.020411   \n",
       "25%      -0.000422        0.000004      0.000690      -0.002545    -0.003317   \n",
       "50%       0.000387        0.000953      0.001564      -0.000901    -0.000558   \n",
       "75%       0.001099        0.002305      0.002397       0.000387     0.000921   \n",
       "max       0.007843        0.059302      0.006537       0.036149     0.009424   \n",
       "\n",
       "       High_15_target  High_15_pred  Low_20_target  Low_20_pred  \\\n",
       "count     7916.000000   7916.000000    7916.000000  7916.000000   \n",
       "mean         0.001353      0.003134      -0.001293     0.001908   \n",
       "std          0.003395      0.001288       0.003944     0.001404   \n",
       "min         -0.043833     -0.003564      -0.059140    -0.006530   \n",
       "25%         -0.000317      0.002366      -0.002748     0.001175   \n",
       "50%          0.000942      0.003112      -0.000918     0.001975   \n",
       "75%          0.002582      0.003919       0.000635     0.002713   \n",
       "max          0.045371      0.009758       0.036223     0.006102   \n",
       "\n",
       "       High_20_target  High_20_pred  \n",
       "count     7916.000000   7916.000000  \n",
       "mean         0.001344      0.000684  \n",
       "std          0.003954      0.001148  \n",
       "min         -0.039862     -0.005927  \n",
       "25%         -0.000544      0.000067  \n",
       "50%          0.000937      0.000789  \n",
       "75%          0.002813      0.001416  \n",
       "max          0.053114      0.004334  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "887e785c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Low_5_target</th>\n",
       "      <th>Low_5_pred</th>\n",
       "      <th>High_5_target</th>\n",
       "      <th>High_5_pred</th>\n",
       "      <th>Low_10_target</th>\n",
       "      <th>Low_10_pred</th>\n",
       "      <th>High_10_target</th>\n",
       "      <th>High_10_pred</th>\n",
       "      <th>Low_15_target</th>\n",
       "      <th>Low_15_pred</th>\n",
       "      <th>High_15_target</th>\n",
       "      <th>High_15_pred</th>\n",
       "      <th>Low_20_target</th>\n",
       "      <th>Low_20_pred</th>\n",
       "      <th>High_20_target</th>\n",
       "      <th>High_20_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.001336</td>\n",
       "      <td>-0.000951</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>-0.001328</td>\n",
       "      <td>-0.002176</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>-0.001297</td>\n",
       "      <td>-0.001826</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>-0.001293</td>\n",
       "      <td>-0.000968</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.001091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>0.001348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.031683</td>\n",
       "      <td>-0.008371</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.008299</td>\n",
       "      <td>-0.050941</td>\n",
       "      <td>-0.008366</td>\n",
       "      <td>-0.016854</td>\n",
       "      <td>-0.010702</td>\n",
       "      <td>-0.066908</td>\n",
       "      <td>-0.008402</td>\n",
       "      <td>-0.043833</td>\n",
       "      <td>-0.016029</td>\n",
       "      <td>-0.059140</td>\n",
       "      <td>-0.005423</td>\n",
       "      <td>-0.039862</td>\n",
       "      <td>-0.014011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.001732</td>\n",
       "      <td>-0.001642</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.002212</td>\n",
       "      <td>-0.002797</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>-0.002545</td>\n",
       "      <td>-0.002443</td>\n",
       "      <td>-0.000317</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>-0.002748</td>\n",
       "      <td>-0.001364</td>\n",
       "      <td>-0.000544</td>\n",
       "      <td>0.000419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000888</td>\n",
       "      <td>-0.000991</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>-0.000915</td>\n",
       "      <td>-0.001867</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>-0.000901</td>\n",
       "      <td>-0.001818</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>-0.000918</td>\n",
       "      <td>-0.000897</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.001212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.000365</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>-0.001300</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.002429</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>-0.001214</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>-0.000533</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>0.001866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.043472</td>\n",
       "      <td>0.009484</td>\n",
       "      <td>0.025916</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.059302</td>\n",
       "      <td>0.019091</td>\n",
       "      <td>0.036149</td>\n",
       "      <td>0.003981</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>0.036223</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.053114</td>\n",
       "      <td>0.008930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Low_5_target   Low_5_pred  High_5_target  High_5_pred  Low_10_target  \\\n",
       "count   7916.000000  7916.000000    7916.000000  7916.000000    7916.000000   \n",
       "mean      -0.001336    -0.000951       0.001387     0.000600      -0.001328   \n",
       "std        0.001608     0.001137       0.001755     0.001075       0.002745   \n",
       "min       -0.031683    -0.008371      -0.000036    -0.008299      -0.050941   \n",
       "25%       -0.001732    -0.001642       0.000389    -0.000012      -0.002212   \n",
       "50%       -0.000888    -0.000991       0.000913     0.000603      -0.000915   \n",
       "75%       -0.000365    -0.000300       0.001784     0.001230       0.000078   \n",
       "max        0.000047     0.009434       0.043472     0.009484       0.025916   \n",
       "\n",
       "       Low_10_pred  High_10_target  High_10_pred  Low_15_target  Low_15_pred  \\\n",
       "count  7916.000000     7916.000000   7916.000000    7916.000000  7916.000000   \n",
       "mean     -0.002176        0.001413      0.001735      -0.001297    -0.001826   \n",
       "std       0.001256        0.002813      0.001412       0.003449     0.001012   \n",
       "min      -0.008366       -0.016854     -0.010702      -0.066908    -0.008402   \n",
       "25%      -0.002797        0.000004      0.000930      -0.002545    -0.002443   \n",
       "50%      -0.001867        0.000953      0.001648      -0.000901    -0.001818   \n",
       "75%      -0.001300        0.002305      0.002429       0.000387    -0.001214   \n",
       "max       0.001518        0.059302      0.019091       0.036149     0.003981   \n",
       "\n",
       "       High_15_target  High_15_pred  Low_20_target  Low_20_pred  \\\n",
       "count     7916.000000   7916.000000    7916.000000  7916.000000   \n",
       "mean         0.001353      0.000720      -0.001293    -0.000968   \n",
       "std          0.003395      0.001581       0.003944     0.000675   \n",
       "min         -0.043833     -0.016029      -0.059140    -0.005423   \n",
       "25%         -0.000317      0.000089      -0.002748    -0.001364   \n",
       "50%          0.000942      0.000940      -0.000918    -0.000897   \n",
       "75%          0.002582      0.001644       0.000635    -0.000533   \n",
       "max          0.045371      0.008983       0.036223     0.002996   \n",
       "\n",
       "       High_20_target  High_20_pred  \n",
       "count     7916.000000   7916.000000  \n",
       "mean         0.001344      0.001091  \n",
       "std          0.003954      0.001348  \n",
       "min         -0.039862     -0.014011  \n",
       "25%         -0.000544      0.000419  \n",
       "50%          0.000937      0.001212  \n",
       "75%          0.002813      0.001866  \n",
       "max          0.053114      0.008930  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e89f4af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_0_target</th>\n",
       "      <th>cluster_0_pred</th>\n",
       "      <th>cluster_1_target</th>\n",
       "      <th>cluster_1_pred</th>\n",
       "      <th>cluster_2_target</th>\n",
       "      <th>cluster_2_pred</th>\n",
       "      <th>cluster_3_target</th>\n",
       "      <th>cluster_3_pred</th>\n",
       "      <th>cluster_4_target</th>\n",
       "      <th>cluster_4_pred</th>\n",
       "      <th>...</th>\n",
       "      <th>cluster_15_target</th>\n",
       "      <th>cluster_15_pred</th>\n",
       "      <th>cluster_16_target</th>\n",
       "      <th>cluster_16_pred</th>\n",
       "      <th>cluster_17_target</th>\n",
       "      <th>cluster_17_pred</th>\n",
       "      <th>cluster_18_target</th>\n",
       "      <th>cluster_18_pred</th>\n",
       "      <th>cluster_19_target</th>\n",
       "      <th>cluster_19_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.00000</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.046999</td>\n",
       "      <td>0.04577</td>\n",
       "      <td>0.043083</td>\n",
       "      <td>0.044481</td>\n",
       "      <td>0.064624</td>\n",
       "      <td>0.055648</td>\n",
       "      <td>0.072142</td>\n",
       "      <td>0.069207</td>\n",
       "      <td>0.065698</td>\n",
       "      <td>0.061821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069109</td>\n",
       "      <td>0.070583</td>\n",
       "      <td>0.047505</td>\n",
       "      <td>0.048819</td>\n",
       "      <td>0.049337</td>\n",
       "      <td>0.050004</td>\n",
       "      <td>0.038155</td>\n",
       "      <td>0.042331</td>\n",
       "      <td>0.041883</td>\n",
       "      <td>0.044945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.211644</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.203050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.247761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200327</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.04577</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055648</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069207</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061821</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.070583</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.048819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.042331</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cluster_0_target  cluster_0_pred  cluster_1_target  cluster_1_pred  \\\n",
       "count      15830.000000     15830.00000      15830.000000    15830.000000   \n",
       "mean           0.046999         0.04577          0.043083        0.044481   \n",
       "std            0.211644         0.00000          0.203050        0.000000   \n",
       "min            0.000000         0.04577          0.000000        0.044481   \n",
       "25%            0.000000         0.04577          0.000000        0.044481   \n",
       "50%            0.000000         0.04577          0.000000        0.044481   \n",
       "75%            0.000000         0.04577          0.000000        0.044481   \n",
       "max            1.000000         0.04577          1.000000        0.044481   \n",
       "\n",
       "       cluster_2_target  cluster_2_pred  cluster_3_target  cluster_3_pred  \\\n",
       "count      15830.000000    15830.000000      15830.000000    15830.000000   \n",
       "mean           0.064624        0.055648          0.072142        0.069207   \n",
       "std            0.245869        0.000000          0.258730        0.000000   \n",
       "min            0.000000        0.055648          0.000000        0.069207   \n",
       "25%            0.000000        0.055648          0.000000        0.069207   \n",
       "50%            0.000000        0.055648          0.000000        0.069207   \n",
       "75%            0.000000        0.055648          0.000000        0.069207   \n",
       "max            1.000000        0.055648          1.000000        0.069207   \n",
       "\n",
       "       cluster_4_target  cluster_4_pred  ...  cluster_15_target  \\\n",
       "count      15830.000000    15830.000000  ...       15830.000000   \n",
       "mean           0.065698        0.061821  ...           0.069109   \n",
       "std            0.247761        0.000000  ...           0.253648   \n",
       "min            0.000000        0.061821  ...           0.000000   \n",
       "25%            0.000000        0.061821  ...           0.000000   \n",
       "50%            0.000000        0.061821  ...           0.000000   \n",
       "75%            0.000000        0.061821  ...           0.000000   \n",
       "max            1.000000        0.061821  ...           1.000000   \n",
       "\n",
       "       cluster_15_pred  cluster_16_target  cluster_16_pred  cluster_17_target  \\\n",
       "count     15830.000000       15830.000000     15830.000000       15830.000000   \n",
       "mean          0.070583           0.047505         0.048819           0.049337   \n",
       "std           0.000000           0.212723         0.000000           0.216577   \n",
       "min           0.070583           0.000000         0.048819           0.000000   \n",
       "25%           0.070583           0.000000         0.048819           0.000000   \n",
       "50%           0.070583           0.000000         0.048819           0.000000   \n",
       "75%           0.070583           0.000000         0.048819           0.000000   \n",
       "max           0.070583           1.000000         0.048819           1.000000   \n",
       "\n",
       "       cluster_17_pred  cluster_18_target  cluster_18_pred  cluster_19_target  \\\n",
       "count     15830.000000       15830.000000     15830.000000       15830.000000   \n",
       "mean          0.050004           0.038155         0.042331           0.041883   \n",
       "std           0.000000           0.191577         0.000000           0.200327   \n",
       "min           0.050004           0.000000         0.042331           0.000000   \n",
       "25%           0.050004           0.000000         0.042331           0.000000   \n",
       "50%           0.050004           0.000000         0.042331           0.000000   \n",
       "75%           0.050004           0.000000         0.042331           0.000000   \n",
       "max           0.050004           1.000000         0.042331           1.000000   \n",
       "\n",
       "       cluster_19_pred  \n",
       "count     15830.000000  \n",
       "mean          0.044945  \n",
       "std           0.000000  \n",
       "min           0.044945  \n",
       "25%           0.044945  \n",
       "50%           0.044945  \n",
       "75%           0.044945  \n",
       "max           0.044945  \n",
       "\n",
       "[8 rows x 40 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591a5b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened\n",
      "{'id': 1, 'result': None}\n",
      "{'e': 'kline', 'E': 1668449991549, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111246, 'o': '16565.00', 'c': '16570.70', 'h': '16571.30', 'l': '16560.40', 'v': '85.200', 'n': 1135, 'x': False, 'q': '1411466.60750', 'V': '45.768', 'Q': '758207.34980', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449992061, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111251, 'o': '16565.00', 'c': '16571.30', 'h': '16571.30', 'l': '16560.40', 'v': '85.233', 'n': 1140, 'x': False, 'q': '1412013.44490', 'V': '45.801', 'Q': '758754.18720', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449992460, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111255, 'o': '16565.00', 'c': '16571.30', 'h': '16571.30', 'l': '16560.40', 'v': '85.340', 'n': 1144, 'x': False, 'q': '1413786.57030', 'V': '45.871', 'Q': '759914.17820', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449993090, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111258, 'o': '16565.00', 'c': '16571.30', 'h': '16571.30', 'l': '16560.40', 'v': '85.586', 'n': 1147, 'x': False, 'q': '1417863.09530', 'V': '45.969', 'Q': '761538.16560', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449993469, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111289, 'o': '16565.00', 'c': '16572.90', 'h': '16573.00', 'l': '16560.40', 'v': '87.212', 'n': 1178, 'x': False, 'q': '1444810.09420', 'V': '47.271', 'Q': '783115.54490', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449993769, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111322, 'o': '16565.00', 'c': '16572.30', 'h': '16573.00', 'l': '16560.40', 'v': '90.501', 'n': 1211, 'x': False, 'q': '1499318.33230', 'V': '47.271', 'Q': '783115.54490', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449994404, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111355, 'o': '16565.00', 'c': '16569.80', 'h': '16573.00', 'l': '16560.40', 'v': '94.489', 'n': 1244, 'x': False, 'q': '1565402.61830', 'V': '47.271', 'Q': '783115.54490', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449995039, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111360, 'o': '16565.00', 'c': '16569.90', 'h': '16573.00', 'l': '16560.40', 'v': '96.294', 'n': 1249, 'x': False, 'q': '1595311.28760', 'V': '49.074', 'Q': '812991.07460', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449995506, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111362, 'o': '16565.00', 'c': '16569.80', 'h': '16573.00', 'l': '16560.40', 'v': '96.946', 'n': 1251, 'x': False, 'q': '1606114.79720', 'V': '49.074', 'Q': '812991.07460', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449996298, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111375, 'o': '16565.00', 'c': '16568.60', 'h': '16573.00', 'l': '16560.40', 'v': '97.391', 'n': 1264, 'x': False, 'q': '1613488.17040', 'V': '49.134', 'Q': '813985.19060', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449996752, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111381, 'o': '16565.00', 'c': '16568.50', 'h': '16573.00', 'l': '16560.40', 'v': '98.750', 'n': 1270, 'x': False, 'q': '1636004.89770', 'V': '50.492', 'Q': '836485.34940', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449997214, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111384, 'o': '16565.00', 'c': '16568.50', 'h': '16573.00', 'l': '16560.40', 'v': '98.859', 'n': 1273, 'x': False, 'q': '1637810.86420', 'V': '50.492', 'Q': '836485.34940', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449997542, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111394, 'o': '16565.00', 'c': '16568.60', 'h': '16573.00', 'l': '16560.40', 'v': '100.718', 'n': 1283, 'x': False, 'q': '1668611.78950', 'V': '51.330', 'Q': '850369.83620', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449997840, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111398, 'o': '16565.00', 'c': '16568.50', 'h': '16573.00', 'l': '16560.40', 'v': '101.508', 'n': 1287, 'x': False, 'q': '1681700.94790', 'V': '51.764', 'Q': '857560.60860', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449998858, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111401, 'o': '16565.00', 'c': '16568.60', 'h': '16573.00', 'l': '16560.40', 'v': '101.549', 'n': 1290, 'x': False, 'q': '1682380.25700', 'V': '51.770', 'Q': '857660.02020', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449999196, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111404, 'o': '16565.00', 'c': '16568.60', 'h': '16573.00', 'l': '16560.40', 'v': '101.592', 'n': 1293, 'x': False, 'q': '1683092.70680', 'V': '51.813', 'Q': '858372.47000', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449999448, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111406, 'o': '16565.00', 'c': '16568.60', 'h': '16573.00', 'l': '16560.40', 'v': '101.649', 'n': 1295, 'x': False, 'q': '1684037.11700', 'V': '51.870', 'Q': '859316.88020', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668449999725, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111407, 'o': '16565.00', 'c': '16568.50', 'h': '16573.00', 'l': '16560.40', 'v': '101.776', 'n': 1296, 'x': False, 'q': '1686141.31650', 'V': '51.870', 'Q': '859316.88020', 'B': '0'}}\n",
      "{'e': 'kline', 'E': 1668450000117, 's': 'BTCUSDT', 'k': {'t': 1668449940000, 'T': 1668449999999, 's': 'BTCUSDT', 'i': '1m', 'f': 3072110112, 'L': 3072111413, 'o': '16565.00', 'c': '16568.60', 'h': '16573.00', 'l': '16560.40', 'v': '102.024', 'n': 1302, 'x': True, 'q': '1690250.30540', 'V': '51.879', 'Q': '859465.99760', 'B': '0'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import websocket\n",
    "\n",
    "socket='wss://fstream.binance.com/ws/'\n",
    "a=None\n",
    "def on_open(ws):\n",
    "    print(\"opened\")\n",
    "    subscribe_message = {\n",
    "        \"method\": \"SUBSCRIBE\",\n",
    "        \"params\":\n",
    "        [\n",
    "         \"btcusdt@kline_1m\",\n",
    "         ],\n",
    "         \"id\": 1\n",
    "         }\n",
    "\n",
    "    ws.send(json.dumps(subscribe_message))\n",
    "\n",
    "def on_message(ws, message):\n",
    "    global lt\n",
    "    #print(\"received a message\")\n",
    "    print(json.loads(message))    \n",
    "    msg=json.loads(message)\n",
    "    #print(msg[\"k\"][\"t\"])\n",
    "def on_close(ws):\n",
    "    print(\"closed connection\")        \n",
    "\n",
    "ws = websocket.WebSocketApp(socket, on_open=on_open, on_message=on_message, on_close=on_close)\n",
    "ws.run_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a12c3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
